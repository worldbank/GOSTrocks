{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import rasterio\n",
    "\n",
    "import GOSTrocks.rasterMisc as rMisc\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "sys.path.insert(0, \"../../src\")\n",
    "import GOSTrocks.dataMisc as dMisc\n",
    "import GOSTrocks.mapMisc as mapMisc\n",
    "\n",
    "\n",
    "import warnings\n",
    "from urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "# Suppress only the InsecureRequestWarning from urllib3\n",
    "warnings.simplefilter('ignore', InsecureRequestWarning)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update to version 3.1\n",
    "In the summer of 2025, we received a complete update of the Fathom dataset labelled 3.1. This notebook will generate new VRTs for version 3.1 by manually editing the vrts from version 3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2020-PERCENTILE50-v3.1',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP1_2.6-PERCENTILE50-v3.1',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP2_4.5-PERCENTILE50-v3.1',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP3_7.0-PERCENTILE50-v3.1',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP5_8.5-PERCENTILE50-v3.1']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of all folders in the new fathom 3.1 bucket\n",
    "s3_bucket = \"wbg-geography01\"\n",
    "s3_prefix = \"FATHOM/v31/\"\n",
    "s3 = boto3.client('s3', verify=False)\n",
    "\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=s3_bucket, Prefix=s3_prefix, Delimiter='/')\n",
    "new_fathom_models = []\n",
    "for page in pages:\n",
    "    for prefix in page.get('CommonPrefixes', []):\n",
    "        new_fathom_models.append(prefix.get('Prefix'))\n",
    "\n",
    "new_fathom_models = [f.split(\"/\")[-2] for f in new_fathom_models if f.endswith('v3.1/')]\n",
    "new_fathom_models[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2020-PERCENTILE50-v3.1.vrt',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP1_2.6-PERCENTILE50-v3.1.vrt',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP2_4.5-PERCENTILE50-v3.1.vrt',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP3_7.0-PERCENTILE50-v3.1.vrt',\n",
       " 'FLOOD_MAP-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP5_8.5-PERCENTILE50-v3.1.vrt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a list of vrts to edit\n",
    "s3_prefix = \"FATHOM/\"\n",
    "paginator = s3.get_paginator('list_objects_v2')\n",
    "pages = paginator.paginate(Bucket=s3_bucket, Prefix=s3_prefix, Delimiter='/')\n",
    "vrt_files = []\n",
    "for page in pages:    \n",
    "    for obj in page.get('Contents', []):\n",
    "        key = obj['Key']\n",
    "        if key.endswith('.vrt'):\n",
    "            vrt_files.append(key)\n",
    "\n",
    "vrt_files = [f.split(\"/\")[-1] for f in vrt_files]\n",
    "vrt_files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each new_fathom_model, find the corresponding vrt and edit it\n",
    "''' This isn't working ...\n",
    "for model in new_fathom_models:\n",
    "    model_base = \"-\".join(model.split(\"-\")[1:-1]) \n",
    "    matching_vrts = [v for v in vrt_files if model_base in v]\n",
    "    if len(matching_vrts) == 1:\n",
    "        cur_vrt = matching_vrts[0]\n",
    "        s3_object = s3.get_object(Bucket=s3_bucket, Key=f\"FATHOM/{cur_vrt}\")\n",
    "    \n",
    "        # Read the content of the object and decode it (assuming UTF-8 encoding)\n",
    "        xml_data = s3_object['Body'].read().decode('utf-8')\n",
    "\n",
    "    break\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class generate_vrt_from_template:\n",
    "    def __init__(self, template_vrt, new_dataset, old_dataset=None, new_vrt=None):\n",
    "        self.template_vrt = template_vrt\n",
    "        self.new_dataset = new_dataset\n",
    "        self.old_dataset = os.path.basename(template_vrt)[:-4] if old_dataset is None else old_dataset\n",
    "        self.new_vrt = self.template_vrt.replace(self.old_dataset, self.new_dataset) if new_vrt is None else new_vrt\n",
    "    def update_vrt(self):\n",
    "        tree = ET.parse(self.template_vrt)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for child in root.iter(\"SourceFilename\"):\n",
    "            cur_tif = child.text.split(\"/\")[-1]\n",
    "            child.text = self.new_dataset + \"/\" + cur_tif\n",
    "\n",
    "        tree.write(self.new_vrt, xml_declaration=False)\n",
    "\n",
    "# For each of the new Fathom models, create a new VRT file, based on existing templates\n",
    "out_folder = \"C:/Temp/fathom_vrts_v31/\"\n",
    "if not os.path.exists(out_folder):\n",
    "    os.makedirs(out_folder)\n",
    "\n",
    "standard_template_vrt = os.path.abspath(\"../sample_data/GLOBAL-1ARCSEC-NW_OFFSET-1in500-PLUVIAL-DEFENDED-DEPTH-2080-SSP5_8.5-PERCENTILE50-v3.0.vrt\")\n",
    "coastal_template_vrt = os.path.abspath(\"../sample_data/GLOBAL-1ARCSEC-NW_OFFSET-1in500-COASTAL-UNDEFENDED-DEPTH-2020-PERCENTILE50-v3.0.vrt\")\n",
    "\n",
    "\n",
    "for c_model in tqdm(new_fathom_models):\n",
    "    cur_template = standard_template_vrt\n",
    "    if \"COASTAL\" in c_model:\n",
    "        cur_template = coastal_template_vrt \n",
    "    out_vrt = os.path.join(out_folder, f\"{c_model}.vrt\")\n",
    "    old_dataset = 'v2023/' + c_model.replace(\"v3.1\", \"v3.0\").replace(\"FLOOD_MAP\", \"GLOBAL\")\n",
    "    new_dataset = 'v31/' + c_model\n",
    "    generate_vrt_from_template(cur_template, new_dataset, old_dataset=old_dataset, new_vrt=out_vrt).update_vrt()\n",
    "    s3.upload_file(out_vrt, s3_bucket, f\"FATHOM/{c_model}.vrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a table of all new VRT files\n",
    "vrt_table = []\n",
    "for c_model in new_fathom_models:\n",
    "    vrt_path = f\"s3://{s3_bucket}/FATHOM/{c_model}.vrt\"\n",
    "    c = c_model.split(\"-\")\n",
    "    cur_vals = [c_model, c[3], c[4], c[5], c[7], c[8], vrt_path]\n",
    "    vrt_table.append(cur_vals)\n",
    "    \n",
    "\n",
    "# Write out the table to S3\n",
    "storage_options = {\n",
    "    'client_kwargs': {\n",
    "        'verify': False # Disable SSL verification\n",
    "    }\n",
    "}\n",
    "out_s3_dataframe = f's3://{s3_bucket}/FATHOM/fathom_v31_vrt_table.csv'\n",
    "pd.DataFrame(vrt_table, \n",
    "             columns=[\"model_name\", \"return\", \"type\", \"defended\", \"year\", \"scenario\", \"vrt_path\"]).to_csv(\n",
    "                out_s3_dataframe, \n",
    "                storage_options=storage_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test results\n",
    "iso3 = 'KEN'\n",
    "world_filepath = r'C:\\WBG\\Work\\data\\ADMIN\\NEW_WB_BOUNDS\\FOR_PUBLICATION\\gpkg\\WB_GAD_ADM0_complete.gpkg'\n",
    "world = gpd.read_file(world_filepath)\n",
    "inB = world.loc[world[\"ISO_A3\"] == iso3].copy()\n",
    "\n",
    "temp_s3_path = f\"s3://{s3_bucket}/FATHOM/{c_model}.vrt\"\n",
    "\n",
    "# open the VRT with rasterio and clip to the country boundary\n",
    "with rasterio.Env(GDAL_HTTP_UNSAFESSL=\"YES\"):\n",
    "    with rasterio.open(temp_s3_path) as src:\n",
    "        out_image, out_transform = rMisc.clipRaster(src, inB, crop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with rMisc.create_rasterio_inmemory(out_transform, out_image) as clipped_raster:\n",
    "    mapMisc.static_map_raster(clipped_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rMisc.clipRaster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"wbg-geography01\"\n",
    "s3_prefix = \"FATHOM/v2023/\"\n",
    "s3_out = os.path.join(\"s3://\", s3_bucket, s3_prefix)\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "my_bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "# Find all files already copied\n",
    "all_folders = []\n",
    "for obj in my_bucket.objects.filter(Prefix=s3_prefix):\n",
    "    all_folders.append(obj.key.split(\"/\")[-2])\n",
    "all_folders\n",
    "all_folders = list(set(all_folders))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build list of rasters for generating VRT\n",
    "local_path = os.path.join(\n",
    "    \"v2023\",\n",
    "    \"GLOBAL-1ARCSEC-NW_OFFSET-1in500-PLUVIAL-DEFENDED-DEPTH-2020-PERCENTILE50-v3.0\",\n",
    ")\n",
    "in_folder = os.path.join(template_folder, local_path)  # noqa\n",
    "all_tiffs = [f\"{local_path}/{x}\" for x in os.listdir(in_folder)]\n",
    "\n",
    "with open(os.path.join(template_folder, \"s3_tiffs.txt\"), \"w\") as out:  # noqa\n",
    "    for p in all_tiffs:\n",
    "        out.write(f\"{p}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_folder = \"/home/wb411133/temp\"\n",
    "coastal_template = os.path.join(\n",
    "    template_folder,\n",
    "    \"GLOBAL-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP3_7.0-PERCENTILE50-v3.0.0.vrt\",\n",
    ")\n",
    "other_template = os.path.join(\n",
    "    template_folder,\n",
    "    \"GLOBAL-1ARCSEC-NW_OFFSET-1in500-PLUVIAL-DEFENDED-DEPTH-2020-PERCENTILE50-v3.0.vrt\",\n",
    ")\n",
    "\n",
    "\n",
    "class generate_vrt_from_template:\n",
    "    def __init__(self, template_vrt, new_dataset):\n",
    "        self.template_vrt = template_vrt\n",
    "        self.new_dataset = new_dataset\n",
    "        self.old_dataset = os.path.basename(template_vrt)[:-4]\n",
    "        self.new_vrt = self.template_vrt.replace(self.old_dataset, self.new_dataset)\n",
    "\n",
    "    def update_vrt(self):\n",
    "        tree = ET.parse(self.template_vrt)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        for child in root.iter(\"SourceFilename\"):\n",
    "            child.text = child.text.replace(self.old_dataset, self.new_dataset)\n",
    "\n",
    "        tree.write(self.new_vrt, xml_declaration=False)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "new_ds = \"GLOBAL-1ARCSEC-NW_OFFSET-1in10-COASTAL-DEFENDED-DEPTH-2030-SSP3_7.0-PERCENTILE50-v3.0\"\n",
    "xx = generate_vrt_from_template(template_vrt, new_ds)\n",
    "xx.update_vrt()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create template vrts\n",
    "for new_ds in all_folders:\n",
    "    flood_type = new_ds.split(\"-\")[4]\n",
    "    vrt_template = other_template\n",
    "    if flood_type == \"COASTAL\":\n",
    "        vrt_template = coastal_template\n",
    "    xx = generate_vrt_from_template(vrt_template, new_ds)\n",
    "    xx.update_vrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aws s3 cp . s3://wbg-geography01/FATHOM/ --exclude \"*\" --include \"*.vrt\"  --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect copied VRTs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dMisc.get_fathom_vrts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = \"wbg-geography01\"\n",
    "s3_prefix = \"FATHOM/\"\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "my_bucket = s3.Bucket(s3_bucket)\n",
    "\n",
    "all_vrts = []\n",
    "for o in my_bucket.objects.filter(Prefix=s3_prefix):\n",
    "    if o.key.endswith(\".vrt\"):\n",
    "        print(o.key)\n",
    "        full_vrt_path = f\"s3://{s3_bucket}/{o.key}\"\n",
    "        all_vrts.append(full_vrt_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vrts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vrts[\"FLOOD_TYPE\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vrts = dMisc.get_fathom_vrts(True)\n",
    "all_res = {}\n",
    "for idx, row in all_vrts.iterrows():\n",
    "    # vrt_path = row['PATH']\n",
    "    # xx = rasterio.open(vrt_path)\n",
    "    filename = os.path.basename(row[\"PATH\"])\n",
    "    year = row[\"YEAR\"]\n",
    "    climate_model = row[\"CLIMATE_MODEL\"]\n",
    "    if climate_model == \"PERCENTILE50\":\n",
    "        climate_model = \"CURRENT\"\n",
    "    flood_type = row[\"FLOOD_TYPE\"].lower()\n",
    "    defence = row[\"DEFENCE\"].lower()\n",
    "    ret = row[\"RETURN\"]\n",
    "    label = \"_\".join([flood_type, defence, ret, climate_model, year])\n",
    "    ret = ret.replace(\"in\", \" in \")\n",
    "    if year == \"2020\":\n",
    "        description = f\"Global {defence} {flood_type} flood model based on current climate. Flood depth is measured in cm expected flood depth, based on a {ret} year return period.\"\n",
    "    else:\n",
    "        description = f\"Global {defence} {flood_type} flood model based on {climate_model} climate model for year {year}. Flood depth is measured in cm expected flood depth, based on a {ret} year return period.\"\n",
    "    all_res[label] = {\"description\": description, \"filename\": row[\"PATH\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"fathom_file_descriptions.json\", \"w\") as out_f:\n",
    "    json.dump(all_res, out_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBURGGING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a list of files for gdalbuildvrt\n",
    "folder = \"/home/wb411133/temp/v2023/GLOBAL-1ARCSEC-NW_OFFSET-1in500-PLUVIAL-DEFENDED-DEPTH-2020-PERCENTILE50-v3.0\"\n",
    "all_files = [\n",
    "    f\"v2023/GLOBAL-1ARCSEC-NW_OFFSET-1in500-PLUVIAL-DEFENDED-DEPTH-2020-PERCENTILE50-v3.0/{x}\"\n",
    "    for x in os.listdir(folder)\n",
    "]\n",
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "s2s_ingest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
